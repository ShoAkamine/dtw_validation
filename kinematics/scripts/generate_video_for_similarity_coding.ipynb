{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate videos for each aligned gesture (for real pairs)\n",
    "\n",
    "### Import packages and define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from moviepy import VideoFileClip, clips_array\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "### define path to video file (two levels up from current directory)\n",
    "media_folder = \"../../../05_data/01_interaction/02_media/\"\n",
    "output_folder = '../data/similarity_coding/videos/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate two videos into one to display two videos side by side\n",
    "Now, we will concatenate two videos to display them next to each other. This is to check with the eyes whether aligned gestures look alike and whether the pose was properly estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_file_path = '../data/processed/05_dtw_distance/dtw_distance.csv'\n",
    "dtw_file = pd.read_csv(dtw_file_path)\n",
    "\n",
    "### for each row in the dataframe, extract the pair, comparison_id, and start and end times\n",
    "for i, row in tqdm(dtw_file.iterrows(), total=len(dtw_file)):\n",
    "    pair = f\"{row['pair']:03d}\"\n",
    "    comparison_id = row['comparison_id']\n",
    "    # average_distance = row['average_distance']\n",
    "    speaker_1 = row['speaker_1'].lower()\n",
    "    speaker_2 = row['speaker_2'].lower()\n",
    "    # round_1 = f\"R{int(row['round_1'])}\"\n",
    "    # round_2 = f\"R{int(row['round_2'])}\"\n",
    "    start_time_1 = row['begin_time_1_adj']/1000 # convert to seconds\n",
    "    end_time_1 = row['end_time_1_adj']/1000 # convert to seconds\n",
    "    start_time_2 = row['begin_time_2_adj']/1000 # convert to seconds\n",
    "    end_time_2 = row['end_time_2_adj']/1000 # convert to seconds\n",
    "\n",
    "    ### get the video files for gesture\n",
    "    video_file_1 = media_folder + f'{pair}/processed/{pair}_{speaker_1}.mp4'\n",
    "    video_file_2 = media_folder + f'{pair}/processed/{pair}_{speaker_2}.mp4'\n",
    "\n",
    "    ### load the video file and extract the relevant portion\n",
    "    video_1 = VideoFileClip(video_file_1).subclipped(start_time_1, end_time_1).without_audio()\n",
    "    video_2 = VideoFileClip(video_file_2).subclipped(start_time_2, end_time_2).without_audio()\n",
    "\n",
    "    ### combine the video clips and save the comined video\n",
    "    combined_video = clips_array([[video_1, video_2]])\n",
    "    output_path = output_folder + f'{comparison_id}.mp4'\n",
    "    combined_video.write_videofile(output_path, codec='libx264', logger=None)\n",
    "\n",
    "    ### close the videos\n",
    "    video_1.close()\n",
    "    video_2.close()\n",
    "    combined_video.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
