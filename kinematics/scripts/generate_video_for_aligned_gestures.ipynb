{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate videos for each aligned gesture (for real pairs)\n",
    "\n",
    "### Import packages and define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from moviepy.editor import VideoFileClip, clips_array\n",
    "from tqdm import tqdm\n",
    "\n",
    "### define path to video file (two levels up from current directory)\n",
    "mediapipe_folder = \"../data/mediapipe/output_videos/\"\n",
    "output_folder = '../data/videos/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate two videos into one to display two videos side by side\n",
    "Now, we will concatenate two videos to display them next to each other. This is to check with the eyes whether aligned gestures look alike and whether the pose was properly estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1304/1304 [43:35<00:00,  2.01s/it] \n"
     ]
    }
   ],
   "source": [
    "### prepare the dataframe containing the gesture alignment data\n",
    "# gest_align_file = '../data/elan_annotation/elan_annotation_gestural_alignment_processed.csv'\n",
    "# df_gest_align = pd.read_csv(gest_align_file)\n",
    "\n",
    "dtw_file_path = '../data/processed/05_dtw_distance/dtw_distance.csv'\n",
    "dtw_file = pd.read_csv(dtw_file_path)\n",
    "\n",
    "### for each row in the dataframe, extract the pair, comparison_id, and start and end times\n",
    "for i, row in tqdm(dtw_file.iterrows(), total=len(dtw_file)):\n",
    "    pair = f\"{row['pair']:03d}\"\n",
    "    comparison_id = row['comparison_id']\n",
    "    average_distance = row['average_distance']\n",
    "    speaker_1 = row['speaker_1'].lower()\n",
    "    speaker_2 = row['speaker_2'].lower()\n",
    "    round_1 = f\"R{int(row['round_1'])}\"\n",
    "    round_2 = f\"R{int(row['round_2'])}\"\n",
    "    start_time_1 = row['begin_time_1_adj']/1000 # convert to seconds\n",
    "    end_time_1 = row['end_time_1_adj']/1000 # convert to seconds\n",
    "    start_time_2 = row['begin_time_2_adj']/1000 # convert to seconds\n",
    "    end_time_2 = row['end_time_2_adj']/1000 # convert to seconds\n",
    "\n",
    "    ### get the video files for gesture\n",
    "    video_file_1 = mediapipe_folder + f'{pair}_{speaker_1}.mp4'\n",
    "    video_file_2 = mediapipe_folder + f'{pair}_{speaker_2}.mp4'\n",
    "\n",
    "    ### load the video file and extract the relevant portion\n",
    "    video_1 = VideoFileClip(video_file_1).subclip(start_time_1, end_time_1)\n",
    "    video_2 = VideoFileClip(video_file_2).subclip(start_time_2, end_time_2)\n",
    "\n",
    "    ### combine the video clips and save the comined video\n",
    "    combined_video = clips_array([[video_1, video_2]])\n",
    "    output_path = output_folder + f'{pair}_{comparison_id}_{round_1}{round_2}_{round(average_distance, 2)}.mp4'\n",
    "    combined_video.write_videofile(output_path, codec='libx264', verbose=False, logger=None)\n",
    "\n",
    "    ### close the videos\n",
    "    video_1.close()\n",
    "    video_2.close()\n",
    "    combined_video.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate videos for non-aligned gesture pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1121 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1121/1121 [46:21<00:00,  2.48s/it]\n"
     ]
    }
   ],
   "source": [
    "dtw_file = '../data/processed/06_dtw_distance/dtw_distance_non_aligned_gestures.csv'\n",
    "df_dtw = pd.read_csv(dtw_file)\n",
    "\n",
    "### for each row in the dataframe, extract the pair, comparison_id, and start and end times\n",
    "for i, row in tqdm(df_dtw.iterrows(), total=len(df_dtw)):\n",
    "    pair = row['pair_x']\n",
    "    comparison_id = row['comparison_id']\n",
    "    average_distance = row['average_distance']\n",
    "    speaker_1 = row['speaker_1']\n",
    "    speaker_2 = row['speaker_2']\n",
    "    round_1 = row['round_1']\n",
    "    round_2 = row['round_2']\n",
    "    referent_1 = row['A_gesture_referent_1'] if row['A_gesture_referent_1'] != \"\" else row['B_gesture_referent_1']\n",
    "    referent_2 = row['A_gesture_referent_2'] if not row['A_gesture_referent_2'] != \"\" else row['B_gesture_referent_2']\n",
    "    start_time_1 = row['begin_time_1']/1000 # convert to seconds\n",
    "    end_time_1 = row['end_time_1']/1000 # convert to seconds\n",
    "    start_time_2 = row['begin_time_2']/1000 # convert to seconds\n",
    "    end_time_2 = row['end_time_2']/1000 # convert to seconds\n",
    "\n",
    "    ### get the video files for gesture\n",
    "    video_file_1 = mediapipe_folder + f'{pair}_synced_pp{speaker_1}.mp4'\n",
    "    video_file_2 = mediapipe_folder + f'{pair}_synced_pp{speaker_2}.mp4'\n",
    "\n",
    "    ### load the video file and extract the relevant portion\n",
    "    video_1 = VideoFileClip(video_file_1).subclip(start_time_1, end_time_1)\n",
    "    video_2 = VideoFileClip(video_file_2).subclip(start_time_2, end_time_2)\n",
    "\n",
    "    ### combine the video clips and save the comined video\n",
    "    combined_video = clips_array([[video_1, video_2]])\n",
    "    output_path = output_non_aligned_folder + f'{pair}_{comparison_id}_{round_1}{round_2}_{referent_1}{referent_2}_{round(average_distance, 2)}.mp4'\n",
    "    combined_video.write_videofile(output_path, codec='libx264', verbose=False, logger=None)\n",
    "\n",
    "    ### close the videos\n",
    "    video_1.close()\n",
    "    video_2.close()\n",
    "    combined_video.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
