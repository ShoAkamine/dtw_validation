{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate videos for each aligned gesture (for real pairs)\n",
    "\n",
    "### Import packages and define paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from moviepy.editor import VideoFileClip, clips_array\n",
    "from tqdm import tqdm\n",
    "\n",
    "### define path to video file (two levels up from current directory)\n",
    "mediapipe_folder = '../../../3_data/referential_task/mediapipe/output_videos/'\n",
    "output_real_folder = '../videos_for_aligned_gestures/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate two videos into one to display two videos side by side\n",
    "Now, we will concatenate two videos to display them next to each other. This is to check with the eyes whether aligned gestures look alike and whether the pose was properly estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 419/419 [18:14<00:00,  2.61s/it]\n"
     ]
    }
   ],
   "source": [
    "### prepare the dataframe containing the gesture alignment data\n",
    "gest_align_file = '../elan_annotation/gesture_form_similarity_coding_processed.csv'\n",
    "df_gest_align = pd.read_csv(gest_align_file)\n",
    "\n",
    "dtw_file_real = '../processed/10_dtw_distance_modified/dtw_distance.csv'\n",
    "df_dtw_real = pd.read_csv(dtw_file_real)\n",
    "# keep the pair_x, comparison_id, and average_distance columns only\n",
    "df_dtw_real = df_dtw_real[['comparison_id', 'average_distance']]\n",
    "\n",
    "# add average_distance to df_gest_align\n",
    "df_gest_align = df_gest_align.merge(df_dtw_real, on=['comparison_id'], how='left')\n",
    "\n",
    "\n",
    "### for each row in the dataframe, extract the pair, comparison_id, and start and end times\n",
    "for i, row in tqdm(df_gest_align.iterrows(), total=len(df_gest_align)):\n",
    "    pair = \"pair\" + str(row['pairnr']).zfill(2)\n",
    "    comparison_id = row['comparison_id']\n",
    "    speaker_1 = row['speaker_1']\n",
    "    speaker_2 = row['speaker_2']\n",
    "    rounds = row['round']\n",
    "    similarity_coding = row['gesture_similarity_coding'][4:8] # only keep the similarity_coding for shape and movement\n",
    "    average_distance = row['average_distance']\n",
    "\n",
    "    if speaker_1 == \"A\":\n",
    "        start_time_1 = row['A_begin_msec_adj']/1000\n",
    "        end_time_1 = row['A_end_msec_adj']/1000\n",
    "        start_time_2 = row['B_begin_msec_adj']/1000\n",
    "        end_time_2 = row['B_end_msec_adj']/1000\n",
    "    elif speaker_1 == \"B\":\n",
    "        start_time_1 = row['B_begin_msec_adj']/1000\n",
    "        end_time_1 = row['B_end_msec_adj']/1000\n",
    "        start_time_2 = row['A_begin_msec_adj']/1000\n",
    "        end_time_2 = row['A_end_msec_adj']/1000\n",
    "\n",
    "    # if speaker_1 == \"A\":\n",
    "    #     start_time_1 = row['A_begin_msec']/1000\n",
    "    #     end_time_1 = row['A_end_msec']/1000\n",
    "    #     start_time_2 = row['B_begin_msec']/1000\n",
    "    #     end_time_2 = row['B_end_msec']/1000\n",
    "    # elif speaker_1 == \"B\":\n",
    "    #     start_time_1 = row['B_begin_msec']/1000\n",
    "    #     end_time_1 = row['B_end_msec']/1000\n",
    "    #     start_time_2 = row['A_begin_msec']/1000\n",
    "    #     end_time_2 = row['A_end_msec']/1000\n",
    "\n",
    "    ### get the video files for gesture\n",
    "    video_file_1 = mediapipe_folder + f'{pair}_synced_pp{speaker_1}.mp4'\n",
    "    video_file_2 = mediapipe_folder + f'{pair}_synced_pp{speaker_2}.mp4'\n",
    "\n",
    "    ### load the video file and extract the relevant portion\n",
    "    video_1 = VideoFileClip(video_file_1).subclip(start_time_1, end_time_1)\n",
    "    video_2 = VideoFileClip(video_file_2).subclip(start_time_2, end_time_2)\n",
    "\n",
    "    ### combine the video clips and save the comined video\n",
    "    combined_video = clips_array([[video_1, video_2]])\n",
    "    output_path = output_real_folder + f'{pair}_{comparison_id}_{rounds}_{similarity_coding}_{round(average_distance, 2)}.mp4'\n",
    "    combined_video.write_videofile(output_path, codec='libx264', verbose=False, logger=None)\n",
    "\n",
    "    ### close the videos\n",
    "    video_1.close()\n",
    "    video_2.close()\n",
    "    combined_video.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
